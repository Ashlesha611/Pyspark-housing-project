
Note: The source CSV file is located in '/housing_data'.
          Update the source_path variable in the .py script if your file is located elsewhere.




# test to create table

# Imports

from pyspark.sql import SparkSession
from pyspark.sql.types import *
from delta.tables import DeltaTable


def create_delta_table():
    """
    Creates a Delta table for the housing dataset using the specified schema.

    Returns:
        None
    """
    # Create a SparkSession
    spark = SparkSession.builder.appName("Create Delta Table").getOrCreate()

    # Define the schema for the housing dataset
   housing_schema = StructType([StructField("longitude",DoubleType(),True),
StructField("latitude", DoubleType(),True),
StructField("housingMedianAge",DoubleType(),True),StructField("totalRooms",DoubleType(),True),
StructField("totalBedrooms",DoubleType(),True),StructField("population",DoubleType(), True),
StructField("households",DoubleType(),True),StructField("medianIncome",DoubleType(),True),
StructField("medianHouseValue",DoubleType(),True)])


    # Create the Delta table
    DeltaTable.createOrReplace(spark, "housing-dataset").schema(housing_schema)

    # Stop the SparkSession
    spark.stop()



# test to ingest data 


  def ingest_data():
    """
    Ingests data from the specified source into the 'housing-dataset' Delta table.

    Returns:
        None
    """
    

 # Create a SparkSession
    
    spark = SparkSession.builder.appName("Data Ingestion").getOrCreate()

    # Read the housing dataset from the source

    housing_df = spark.read.format("csv").load("housing_data")



    # Write the housing dataset to the Delta table

    housing_df.write.format("delta").mode("overwrite").save("housing-dataset")



    # Stop the SparkSession
    spark.stop()



